\begin{frame}
		\begin{figure}
				\includegraphics[width=0.8\linewidth]{figures/diagrams/splitmem/memsplit}
		\end{figure}
\end{frame}

\begin{frame}
		\begin{figure}
			\includegraphics[width=0.3\linewidth]{figures/diagrams/sort/serialsort}
		\end{figure}
\end{frame}

\begin{frame}
		\begin{figure}
				\includegraphics[width=0.8\linewidth]{figures/diagrams/sort/parallelsort}
		\end{figure}
\end{frame}

\begin{frame}
		mpirun - CLI mpi examples
\end{frame}

\begin{frame}[fragile]
		\frametitle{R 'pbdMPI' package example}
		\begin{verbatim}
		library(pbdMPI)

		init()
		.comm.size <- comm.size()
		.comm.rank <- comm.rank()

		x <- 5

		if (.comm.rank == 0) {
		    send(x)
		} else {
		    y <- recv(x)
		}

		comm.print(y,rank.print=1)
		finalize()
		\end{verbatim}
\end{frame}

\begin{frame}[fragile]
		\frametitle{Another MPI example}
		\begin{verbatim}
		init()
		.comm.size <- comm.size()
		.comm.rank <- comm.rank()

		x <- sample(1:1000, 50, replace = T)

		x_global <- 0
		y <- sum(x)

		x_global <- reduce(y, op="sum")

		comm.print(x_global, rank.print==0)
		finalize()
		\end{verbatim}
\end{frame}

\begin{frame}
		observed speedup ( figure )
\end{frame}

\begin{frame}
		Does not reduce memory usage (diagram)
\end{frame}

\begin{frame}
		MPI vs Shared
\end{frame}

\begin{frame}
		infrastructure limits ( figure )
\end{frame}


